{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using nltk\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import glob\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# in charge of spliting dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#importing dataset which divides into data and target sets\n",
    "from sklearn.datasets import load_files  \n",
    "\n",
    "#regex for preprocessing\n",
    "import re\n",
    "\n",
    "#lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "#Stemming\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "\n",
    "#ploting graph\n",
    "# import seaborn as sns\n",
    "\n",
    "#serialization\n",
    "# A common pattern in Python 2.x is to have one version of a module implemented \n",
    "# in pure Python, with an optional accelerated version implemented as a C extension; \n",
    "#for example, pickle\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataSentences</th>\n",
       "      <th>dataLabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dataSentences  dataLabels\n",
       "0  A very, very, very slow-moving, aimless movie ...           0\n",
       "1  Not sure who was more lost - the flat characte...           0\n",
       "2  Attempting artiness with black & white and cle...           0\n",
       "3       Very little music or anything to speak of.             0\n",
       "4  The best scene in the movie was when Gerardo i...           1"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReviews = pd.read_csv('allreviews.txt', \n",
    "                          delimiter=\"\\t\", \n",
    "                          names=['dataSentences', \n",
    "                                 'dataLabels'])\n",
    "\n",
    "review_x, review_y =dataReviews['dataSentences'], dataReviews['dataLabels']\n",
    "\n",
    "# for gt in review_x:\n",
    "#     print (gt)\n",
    "\n",
    "#shows a representation of data sentences and corresponding labels\n",
    "dataReviews.head()\n",
    "\n",
    "# sns.countplot(x='Sentiment',y='count', data=dataReviews)\n",
    "\n",
    "# print(type(review_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def preprocessing(reviews):\n",
    "   \n",
    "    preprocessedDoc = []\n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for sentence in range(0, len(reviews)):  \n",
    "    #     print(review_x[sentence])\n",
    "        # Remove all the special characters\n",
    "        a = re.sub(r'\\W', ' ', str(reviews[sentence]))\n",
    "\n",
    "        # remove all single characters\n",
    "        a = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', a)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        a = re.sub(r'\\^[a-zA-Z]\\s+', ' ', a) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        a = re.sub(r'\\s+', ' ', a, flags=re.I)\n",
    "        \n",
    "        # Converting to lowercase\n",
    "#         a = a.str.lower()\n",
    "                     \n",
    "        preprocessedDoc.append(a)\n",
    "        \n",
    "    preprocessedDoc = [[wordnet_lemmatizer.lemmatize(word) for word in word_tokenize(s)]\n",
    "              for s in preprocessedDoc]\n",
    "\n",
    "    return preprocessedDoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepre(review):\n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    a = re.sub(r'\\W', ' ', str(review))\n",
    "\n",
    "    # remove all single characters\n",
    "    a = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', a)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    a = re.sub(r'\\^[a-zA-Z]\\s+', ' ', a) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    a = re.sub(r'\\s+', ' ', a, flags=re.I)\n",
    "    \n",
    "    r = [wordnet_lemmatizer.lemmatize(word) for word in word_tokenize(review)]\n",
    "    \n",
    "    return r\n",
    "\n",
    "# prepre(\"review_x was really bad and boring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalised : split dataset into training data and testing data\n",
    "\n",
    "# # Converting preprocessedDoc type:list to numpy array\n",
    "# numpy_preprocessedDoc = np.asarray(preprocessing(review_x))\n",
    "\n",
    "# Converting preprocessedDoc type:list to pandas.core.series.Series\n",
    "pandas_preprocessedDoc =  pd.Series(preprocessing(review_x)).astype(str).str.zfill(11)\n",
    "\n",
    "# print(type(pandas_preprocessedDoc))\n",
    "\n",
    "x_NormTrain, x_NormTest, y_NormTrain, y_NormTest = train_test_split(\n",
    "                                                    pandas_preprocessedDoc, \n",
    "                                                     review_y, \n",
    "                                                    test_size=0.05,\n",
    "                                                    train_size = 0.95,\n",
    "                                                    random_state = 42) \n",
    "\n",
    "#UnNormalised : split dataset into training data and testing data\n",
    "x_UnNormTrain, x_UnNormTest, y_UnNormTrain, y_UnNormTest = train_test_split(\n",
    "                                                    review_x, \n",
    "                                                     review_y, \n",
    "                                                    test_size=0.05,\n",
    "                                                    train_size = 0.95,\n",
    "                                                    random_state = 42)      \n",
    "# print (X_UnNormTrain)\n",
    "# print(a.reshape(a.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2325, 4308)\n",
      "(2325,)\n",
      "--------LOGISTIC REGRESSION NORMALIZED----------\n",
      "ACCURACY SCORE:  82.11382113821138\n",
      "\n",
      "CONTINGENCY TABLE : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85        71\n",
      "           1       0.80      0.77      0.78        52\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       123\n",
      "   macro avg       0.82      0.81      0.82       123\n",
      "weighted avg       0.82      0.82      0.82       123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sasha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# In this funtion, we train for logistic regression classifier\n",
    "#We use the bag of words approach for convertting the text to numbers.\n",
    "#To resolve the issue of the bag of words approach not taking into account \n",
    "#that the word might also be having a high frequency of occurrence in other \n",
    "#documents as well.\n",
    "\n",
    "def logisticRegTrainNorm():\n",
    "    \n",
    "    #Normalisation of text \n",
    "    stopSet = set(stopwords.words('english'))\n",
    "    \n",
    "    #Term-frequency times document frequency Vectorizer was used because \n",
    "    #it does not only provide the number of occurrences but also tells\n",
    "    #about the importance of the word.\n",
    "    tfid_vectorizer_n_lr = TfidfVectorizer(\n",
    "                                      use_idf=True, # IDF is \"t\" when use_idf is given\n",
    "                                      smooth_idf = True, #adds \"1\" to the numerator and denominator\n",
    "#                                       lowercase=True, \n",
    "                                      strip_accents=ascii, \n",
    "                                      stop_words=stopSet\n",
    "                                      )\n",
    "    \n",
    "    tfid_vectorizer_n_lr.fit(x_NormTrain)\n",
    "    norm_x_lg = tfid_vectorizer_n_lr.transform(x_NormTrain).toarray()  \n",
    "    print(norm_x_lg.shape)\n",
    "    print(y_NormTrain.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # Training the normalized Logistic Regression Classifier\n",
    "    logistic_norm_classifier = LogisticRegression().fit(norm_x_lg,y_NormTrain)\n",
    "    \n",
    "#     testing\n",
    "    x_test_features_lr_n = tfid_vectorizer_n_lr.transform(x_NormTest).toarray()\n",
    "    x_predict_lr_n = logistic_norm_classifier.predict(x_test_features_lr_n)\n",
    "    \n",
    "    print(\"--------LOGISTIC REGRESSION NORMALIZED----------\")\n",
    "    \n",
    "    print(\"ACCURACY SCORE: \", accuracy_score(y_NormTest, x_predict_lr_n)*100) \n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(\"CONTINGENCY TABLE : \\n\" ,classification_report(y_NormTest, x_predict_lr_n))\n",
    "        \n",
    "    return logistic_norm_classifier, tfid_vectorizer_n_lr \n",
    "\n",
    "\n",
    "logistic_norm_classifier, tfid_vectorizer_n_lr = logisticRegTrainNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------LOGISTIC REGRESSION UNNORMALIZED----------\n",
      "ACCURACY SCORE:  81.30081300813008\n",
      "\n",
      "CONTINGENCY TABLE: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.84        71\n",
      "           1       0.77      0.79      0.78        52\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       123\n",
      "   macro avg       0.81      0.81      0.81       123\n",
      "weighted avg       0.81      0.81      0.81       123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sasha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def logisticRegTrainUnNorm():\n",
    "    \n",
    "    #Term-frequency times document frequency Vectorizer was used because \n",
    "    #it does not only provide the number of occurrences but also tells\n",
    "    #about the importance of the word.\n",
    "    \n",
    "    #to get unnormalized version of logistic regression set parameters to\n",
    "    #False and None\n",
    "    tfid_vectorizer_un_lr = TfidfVectorizer(\n",
    "                                      use_idf=True,\n",
    "                                      smooth_idf = True\n",
    "                                      )\n",
    "\n",
    "    unNorm_x_lg = tfid_vectorizer_un_lr.fit_transform(x_UnNormTrain).toarray()    \n",
    "\n",
    "    # Training the normalized Logistic Regression Classifierr\n",
    "    logistic_un_classifier = LogisticRegression().fit(unNorm_x_lg, y_UnNormTrain)\n",
    "    \n",
    "    \n",
    "    # testing\n",
    "    x_test_features_lr_un = tfid_vectorizer_un_lr.transform(x_UnNormTest)\n",
    "    x_predict_lr_un = logistic_un_classifier.predict(x_test_features_lr_un)\n",
    "    \n",
    "    print(\"--------LOGISTIC REGRESSION UNNORMALIZED----------\")\n",
    "\n",
    "    print(\"ACCURACY SCORE: \", accuracy_score(y_UnNormTest, x_predict_lr_un)*100) \n",
    "\n",
    "    print()\n",
    "    \n",
    "    print(\"CONTINGENCY TABLE: \\n\" ,classification_report(y_UnNormTest, x_predict_lr_un))\n",
    "    \n",
    "    return logistic_un_classifier, tfid_vectorizer_un_lr \n",
    "\n",
    "logistic_un_classifier, tfid_vectorizer_un_lr = logisticRegTrainUnNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------NAIVE BAYES UNNORMALIZED CLASSIFIER----------\n",
      "ACCURACY SCORE:  77.23577235772358\n",
      "\n",
      "CONTINGENCY TABLE: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        71\n",
      "           1       0.73      0.73      0.73        52\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       123\n",
      "   macro avg       0.77      0.77      0.77       123\n",
      "weighted avg       0.77      0.77      0.77       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def trainUnNormNB():\n",
    "    # Using TFIDF, short for term frequency–inverse document frequency\n",
    "    # This transforms text to feature vectors\n",
    "\n",
    "    #to get unnormalized version of logistic regression set parameters to\n",
    "    #False and None\n",
    "    tfid_vectorizer_un_nb = TfidfVectorizer(\n",
    "                                        use_idf=True, \n",
    "                                        lowercase=False, \n",
    "                                        strip_accents=None, \n",
    "                                        stop_words= None\n",
    "                                       )\n",
    "\n",
    "    unNorm_x_nb = tfid_vectorizer_un_nb.fit_transform(x_UnNormTrain)\n",
    "\n",
    "    # Training the unnormalized Naive Bayes Classifier\n",
    "    unNorm_nbClassifier = naive_bayes.MultinomialNB().fit(unNorm_x_nb, y_UnNormTrain)\n",
    "    \n",
    "       # testing\n",
    "    x_test_features_nb_un = tfid_vectorizer_un_nb.transform(x_UnNormTest)\n",
    "    x_predict_nb_un = unNorm_nbClassifier.predict(x_test_features_nb_un)\n",
    "    \n",
    "    print(\"--------NAIVE BAYES UNNORMALIZED CLASSIFIER----------\")\n",
    "\n",
    "    print(\"ACCURACY SCORE: \", accuracy_score(y_UnNormTest, x_predict_nb_un)*100)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print(\"CONTINGENCY TABLE: \\n\" ,classification_report(y_UnNormTest, x_predict_nb_un))\n",
    "    \n",
    "    return unNorm_nbClassifier, tfid_vectorizer_un_nb\n",
    "\n",
    "unNorm_nbClassifier,tfid_vectorizer_un_nb = trainUnNormNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------NAIVE BAYES NORMALIZED CLASSIFIER----------\n",
      "ACCURACY SCORE:  80.48780487804879\n",
      "\n",
      "CONTINGENCY TABLE: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83        71\n",
      "           1       0.76      0.79      0.77        52\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       123\n",
      "   macro avg       0.80      0.80      0.80       123\n",
      "weighted avg       0.81      0.80      0.81       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def trainNormNB():\n",
    "    \n",
    "    #Normalisation of text \n",
    "    stopSet = set(stopwords.words('english'))\n",
    "    \n",
    "    #Term-frequency times document frequency Vectorizer was used because \n",
    "    #it does not only provide the number of occurrences but also tells\n",
    "    #about the importance of the word.\n",
    "    tfid_vectorizer_n_nb = TfidfVectorizer(\n",
    "                                      use_idf=True, # IDF is \"t\" when use_idf is given\n",
    "                                      smooth_idf = True, #adds \"1\" to the numerator and denominator\n",
    "                                      lowercase=True, \n",
    "                                      strip_accents='ascii', \n",
    "                                      stop_words=stopSet\n",
    "                                      )\n",
    "\n",
    "    norm_x_nb = tfid_vectorizer_n_nb.fit_transform(x_NormTrain).toarray()    \n",
    "\n",
    "\n",
    "    # Training the normalized Logistic Regression Classifier\n",
    "    norm_nbClassifier = naive_bayes.MultinomialNB().fit(norm_x_nb, y_NormTrain)\n",
    "    \n",
    "    # testing classifier\n",
    "    x_test_features_nb_n = tfid_vectorizer_n_nb.transform(x_NormTest)\n",
    "    x_predict_nb_n = norm_nbClassifier.predict(x_test_features_nb_n)\n",
    "    \n",
    "    print(\"--------NAIVE BAYES NORMALIZED CLASSIFIER----------\")\n",
    "\n",
    "    print(\"ACCURACY SCORE: \", accuracy_score(y_NormTest, x_predict_nb_n)*100) \n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print(\"CONTINGENCY TABLE: \\n\" ,classification_report(y_NormTest, x_predict_nb_n))\n",
    "    \n",
    "    return norm_nbClassifier, tfid_vectorizer_n_nb   \n",
    "\n",
    "norm_nbClassifier, tfid_vectorizer_n_nb = trainNormNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Serialization of both classifier and vectorizer is done in here.\n",
    "#In text classification, it is not enough to just store the classfier\n",
    "#therefore the vectorizer is also needed for future usage\n",
    "\n",
    "# # --------LOGISTIC REGRESSION SERIALIZATION----------\n",
    "\n",
    "# with open('logistic_norm_classifier_vectorizer.sav', 'wb') as pickle_lg_n:  \n",
    "#     pickle.dump((logistic_norm_classifier,tfid_vectorizer_n_lr),pickle_lg_n)\n",
    "    \n",
    "# with open('logistic_unNorm_classifier_vectorizer.sav', 'wb') as pickle_lg_un:  \n",
    "#     pickle.dump((logistic_un_classifier,tfid_vectorizer_un_lr),pickle_lg_un)\n",
    "    \n",
    "# # -----------NAIVE BAYES SERIALIZATION----------\n",
    "# with open('nb_unNorm_classifier_vectorizer.sav', 'wb') as pickle_nb_un:  \n",
    "#     pickle.dump((unNorm_nbClassifier,tfid_vectorizer_un_nb),pickle_nb_un)\n",
    "    \n",
    "# with open('nb_norm_classifier_vectorizer.sav', 'wb') as pickle_nb_n:  \n",
    "#     pickle.dump((norm_nbClassifier,tfid_vectorizer_n_nb),pickle_nb_n)  \n",
    "    \n",
    "  \n",
    "\n",
    "# with open('logistic_norm_classifier_vectorizer.sav', 'rb') as load_pickle_lg_n:\n",
    "#       logistic_norm_classifier,tfid_vectorizer_n_lr = pickle.load(load_pickle_lg_n)\n",
    "    \n",
    "#     with open('logistic_unNorm_classifier_vectorizer.sav', 'rb') as load_pickle_lg_un:\n",
    "#       logistic_un_classifier,tfid_vectorizer_un_lr = pickle.load(load_pickle_lg_un)\n",
    "    \n",
    "#     with open('nb_unNorm_classifier_vectorizer.sav', 'rb') as load_pickle_nb_un:\n",
    "#       unNorm_nbClassifier,tfid_vectorizer_un_nb = pickle.load(load_pickle_nb_un)\n",
    "    \n",
    "#     with open('nb_norm_classifier_vectorizer.sav', 'rb') as load_pickle_nb_un:\n",
    "#       norm_nbClassifier,tfid_vectorizer_un_nb = pickle.load(load_pickle_nb_un)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2325, 4308)\n",
      "(2325,)\n",
      "--------LOGISTIC REGRESSION NORMALIZED----------\n",
      "ACCURACY SCORE:  82.11382113821138\n",
      "\n",
      "CONTINGENCY TABLE : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85        71\n",
      "           1       0.80      0.77      0.78        52\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       123\n",
      "   macro avg       0.82      0.81      0.82       123\n",
      "weighted avg       0.82      0.82      0.82       123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sasha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def testingUnNormLog(testdoc, version, typeClassifier):\n",
    "    predict= []\n",
    "\n",
    "    with open (testdoc, \"r\") as openedTestdoc:\n",
    "        if typeClassifier == \"lr\":\n",
    "            if version == \"un\":\n",
    "                logistic_un_classifier, tfid_vectorizer_un_lr = logisticRegTrainUnNorm()\n",
    "                for sentence in openedTestdoc:\n",
    "                    predict.append(sentence.strip('\\r\\n'))\n",
    "                x_test_features_lr_un = tfid_vectorizer_un_lr.transform(predict)\n",
    "                x_predict_lr_un = logistic_un_classifier.predict(x_test_features_lr_un)\n",
    "                \n",
    "#               write results to document\n",
    "                with open(\"result-lr-un\", 'w', newline = '') as r1:\n",
    "                    for prediction_lr_un in x_predict_lr_un:\n",
    "                        r1.write(str(prediction_lr_un) + '\\n')             \n",
    "            \n",
    "            elif version == \"n\":\n",
    "                logistic_norm_classifier, tfid_vectorizer_n_lr = logisticRegTrainNorm ()\n",
    "                for sentence in openedTestdoc:  \n",
    "                    preText= prepre(sentence.lower())\n",
    "                x_test_features_lr_n = tfid_vectorizer_n_lr.transform(preText)\n",
    "                x_predict_lr_n = logistic_norm_classifier.predict(x_test_features_lr_n)\n",
    "                \n",
    "                with open(\"result-lr-n.txt\", 'w', newline = '') as r2:\n",
    "                    for prediction_lr_n in x_predict_lr_n:\n",
    "                        r2.write(str(prediction_lr_n) + '\\n')\n",
    "                \n",
    "            else:\n",
    "                print('Invalid version specified')\n",
    "                \n",
    "        if typeClassifier == \"nb\":\n",
    "            if version == \"un\":\n",
    "                unNorm_nbClassifier, tfid_vectorizer_un_nb = trainUnNormNB ()\n",
    "                for sentence in openedTestdoc:\n",
    "                    predict.append(sentence.strip('\\r\\n'))\n",
    "                x_test_features_nb_un = tfid_vectorizer_un_nb.transform(predict)\n",
    "                x_predict_nb_un = unNorm_nbClassifier.predict(x_test_features_nb_un)\n",
    "                \n",
    "                with open(\"result-nb-un.txt\", 'w', newline = '') as r3:\n",
    "                    for prediction_nb_un in x_predict_nb_un:\n",
    "                        r3.write(str(prediction_nb_un) + '\\n')\n",
    "            \n",
    "            elif version == \"n\":\n",
    "                norm_nbClassifier, tfid_vectorizer_n_nb = trainNormNB()\n",
    "                for sentence in openedTestdoc:\n",
    "                    predict.append(prepre(sentence.lower()))\n",
    "                x_test_features_nb_n = tfid_vectorizer_un_nb.transform(predict)\n",
    "                x_predict_nb_n = norm_nbClassifier.predict(x_test_features_nb_n)\n",
    "\n",
    "                with open(\"result-nb-n.txt\", 'w', newline = '') as r4:\n",
    "                    for prediction_nb_n in x_predict_nb_n:\n",
    "                        r4.write(str(prediction_nb_n) + '\\n')\n",
    "\n",
    "            else:\n",
    "                print('Invalid version specified')\n",
    "\n",
    "testingUnNormLog(\"test_sentences.txt\", \"n\", \"lr\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeClassifier = sys.argv[1]\n",
    "version = sys.argv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "#      # accept command-line arguments\n",
    "#     parser =argparse.ArgumentParser(description=\"Sentiment analysis for reviews using both a Logistic Regression model and a Multinomial Naive Bayes Classification model.\")\n",
    "#     parser.add_argument(\"classifier-type\", help=\"specifies the type of classifier to use. 'nb'=naiveBayes, and 'lr'=logisticRegression\")\n",
    "#     parser.add_argument(\"version\", help=\"specifies which version of classifier to use. 'u'=un-normalized or 'n'=normalized\")\n",
    "#     parser.add_argument(\"testfile\", help=\"accepts the text file to peform sentiment analysis on.\")\n",
    "\n",
    "    \n",
    "#     args = vars(parser.parse_args())\n",
    "     \n",
    "#     # extract arguments passed from command-line\n",
    "#     classifier_type = args.get('classifier-type', None)\n",
    "#     version = args.get('version', None)\n",
    "#     testfile = args.get('testfile', None)\n",
    "    \n",
    "#     testingUnNormLog(testfile,version, classifier_type )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
